[["Map",1,2,9,10,77,78],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.15.9","content-config-digest","d0ea88b6c980dd3c","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://vibhavmisra.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark-dimmed\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","projects",["Map",11,12,29,30,46,47,60,61],"ask-vibhav-resume-chatbot",{"id":11,"data":13,"body":24,"filePath":25,"digest":26,"legacyId":27,"deferredRender":28},{"title":14,"date":15,"summary":16,"tags":17,"repo":22,"demo":23},"Ask-Vibhav Resume Chatbot",["Date","2025-04-10T00:00:00.000Z"],"A GPT-powered chatbot trained on my resume! It answers questions about my background, technical skills, education, and projects, just like an interactive CV.",[18,19,20,21],"Resume","AI","Chatbot","Groq API","https://github.com/Vibhav-Misra/AskVibhav","https://ask-vibhav.vercel.app/","‚óâ An interactive chatbot trained on my resume, deployed on Vercel, powered by Groq LLM API.\r\n\r\n‚óâ Users can ask questions about my background, skills, education, experience, or projects and get fast, on-topic answers directly from the resume.\r\n\r\n---\r\n\r\n‚óâ **Overview**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n- Uses a fixed knowledge base (My resume text) ‚Äì no need for a vector DB.\r\n- Built as a Next.js / Vercel serverless API endpoint (/api/chat).\r\n- Talks to Groq‚Äôs OpenAI-compatible Chat Completions API with modern LLaMA 3 models.\r\n- Front-end is a minimal chat UI deployed along with the backend.\r\n- The project is designed to be cheap to run (no RAG or embeddings) and simple to maintain.\r\n\r\n---\r\n\r\n‚óâ **Architecture & Design**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n- **High-Level Architecture**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n![High-Level Architecture](/images/flowcharts/flowchart2.png)\r\n\u003Cp style=\"margin-top:2rem;\">\u003C/p>\r\n\r\n---\r\n\r\n\u003Cp style=\"margin-top:2rem;\">\u003C/p>\r\n‚óâ **Workflow**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n```bash\r\n# 1. Clone the Repo\r\ngit clone https://github.com/your-handle/ask-vibhav.git\r\ncd ask-vibhav\r\n\r\n# 2. Install Dependencies\r\nnpm install\r\n# or\r\nyarn install\r\n\r\n# 3. Set Environment Variables\r\n# Create a .env.local file (for local dev) and add:\r\nGROQ_API_KEY=your-groq-key-here\r\n\r\n# 4. Configure Node Runtime (important on Vercel)\r\n# Either in package.json:\r\n{ \"engines\": { \"node\": \"20.x\" } }\r\n\r\n# or in vercel.json:\r\n{ \"functions\": { \"api/*.js\": { \"runtime\": \"nodejs20.x\" } } }\r\n\r\n# 5. Run Locally\r\n# open http://localhost:3000\r\nnpm run dev\r\n\r\n# 6. Deploy to Vercel\r\nvercel deploy","src/content/projects/ask-vibhav-resume-chatbot.mdx","219c01f9bbee5fa7","ask-vibhav-resume-chatbot.mdx",true,"exoplanet-habitability-explorer",{"id":29,"data":31,"body":42,"filePath":43,"digest":44,"legacyId":45,"deferredRender":28},{"title":32,"date":33,"summary":34,"tags":35,"repo":40,"demo":41},"Exoplanet Habitability Explorer",["Date","2025-09-30T00:00:00.000Z"],"Interactive exploration of 5,000+ exoplanets (NASA PSCompPars), explainable habitability score + RF classifier, presets, details drawer, and compare tray.",[36,37,38,39],"Streamlit","ML","NASA","Astrophysics","https://github.com/Vibhav-Misra/exoplanet-habitability-explorer","https://huggingface.co/spaces/VibzMiz/exoplanet-habitability-explorer","‚óâ An end-to-end data-science & ML project that explores, ranks, and visualizes thousands of confirmed exoplanets using data from the NASA Exoplanet Archive.\r\n\r\n‚óâ The project demonstrates data sourcing, feature engineering, interactive visualization, and ML classification in a single deployable web app.\r\n\r\n---\r\n\r\n‚óâ **Features**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n- **Live NASA data pull** ‚Äì fetches the latest PSCompPars catalog from the NASA Exoplanet Archive via its TAP API (`src/fetch_data.py`)\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n- **Explainable habitability score** ‚Äì composite of physical parameters such as insolation, radius, distance, stellar temperature, etc.\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n- **Interactive web app (Streamlit)** --\r\n  - Filter by radius, insolation, discovery year, distance, etc.\r\n  - Weight presets (Conservative HZ / Optimistic HZ / Observation-friendly) plus sliders for custom scoring\r\n  - Click-to-inspect planet details with score-component breakdown\r\n  - Compare tray for side-by-side comparison of up to 3 planets\r\n  - Downloadable filtered table\r\n  \u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n- **ML classifier** ‚Äì trains a Random-Forest model to predict *‚Äúoptimistic habitable-zone candidate‚Äù* label from non-leaking astrophysical & engineered features  \r\n  (e.g. luminosity proxy, semi-major axis, estimated insolation)\r\n  - ROC-AUC / PR-AUC / F1 displayed in the app\r\n  - Optional toggle to show predicted probability & label in the UI\r\n  \u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n- **Clean architecture & reproducibility** ‚Äì separate training script, model artifacts in `/models`, Streamlit app in `app.py`\r\n\r\n---\r\n\r\n‚óâ **Tech Stack**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n- **Python:** `pandas`, `numpy`, `requests`, `pyarrow`\r\n- **Data science / ML:** `scikit-learn`, `joblib`\r\n- **Web app / viz:** `Streamlit`, `Plotly`, `streamlit-plotly-events`\r\n- **Data source:** NASA Exoplanet Archive TAP API\r\n\r\n---\r\n\r\n‚óâ **Workflow**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n```bash\r\n# 1. Create environment & install deps\r\npython3 -m venv venv\r\nsource venv/bin/activate\r\npip install -r requirements.txt\r\n\r\n# 2. Pull latest data\r\npython src/fetch_data.py\r\n\r\n# 3. (optional) Train / update ML model\r\npython src/train_classifier.py\r\n\r\n# 4. Run interactive app locally\r\nstreamlit run app.py","src/content/projects/exoplanet-habitability-explorer.mdx","f79383f43fc3f67c","exoplanet-habitability-explorer.mdx","memory-manager-for-ai-chatbot",{"id":46,"data":48,"body":56,"filePath":57,"digest":58,"legacyId":59,"deferredRender":28},{"title":49,"date":50,"summary":51,"tags":52,"repo":55},"Memory Manager for AI Chatbot",["Date","2025-07-25T00:00:00.000Z"],"Designed memory‚Äëbased personalization (episodic + semantic) over MongoDB; improved continuity and engagement.",[19,20,53,54],"MongoDB","Personalization","https://github.com/Vibhav-Misra/memory-manager-for-AI-chatbot","‚óâ A memory management service to allow an AI chatbot to remember conversations or user preferences.\r\n\r\n‚óâ This system automatically extracts, scores, and manages meaningful informationfrom conversations, creating a persistent memory system.\r\n\r\n---\r\n\r\n‚óâ **Features**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n- Automatic memory extraction from conversations.\r\n- AI-powered scoring and decision making.\r\n- Human oversight for uncertain memories.\r\n- MongoDB storage.\r\n- Real-time processing via REST API.\r\n- Admin interface for memory review.\r\n\r\n---\r\n\r\n‚óâ **Architecture & Design**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n- **High-Level Architecture**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n![High-Level Architecture](/images/flowcharts/flowchart1.png)\r\n\r\n---\r\n\r\n‚óâ **Workflow**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n```bash\r\n# 1. Clone/Download the Project\r\n# Navigate to your desired directory\r\ncd /path/to/your/project\r\n\r\n# 2. Install Python Dependencies\r\n# Install all required packages\r\npip install -r requirements.txt\r\n\r\n# 3. Start MongoDB\r\n# On macOS/Linux:\r\nmongod\r\n\r\n# On Windows:\r\n\"C:\\Program Files\\MongoDB\\Server\\8.0\\bin\\mongod.exe\"\r\n\r\n# 4. Create Environment Configuration\r\n# Run the startup script (creates .env file)\r\npython run_service.py\r\n\r\n# 5. Verify Installation\r\n# Test if the service can start\r\npython test_service.py\r\n\r\n# If successful, start the main service\r\npython run_service.py\r\n\r\n# 6. Start Admin Interface (Optional)\r\n# In a new terminal\r\nstreamlit run admin_ui.py","src/content/projects/memory-manager-for-AI-chatbot.mdx","387907b0873f02d0","memory-manager-for-AI-chatbot.mdx","will-i-catch-that-train",{"id":60,"data":62,"body":73,"filePath":74,"digest":75,"legacyId":76,"deferredRender":28},{"title":63,"date":64,"summary":65,"tags":66,"repo":71,"demo":72},"Will I Catch That Train",["Date","2025-08-15T00:00:00.000Z"],"A Streamlit web app that helps me decide when to leave home for the J, Z, M subway lines in NYC.",[67,68,69,70],"Subway","API","Real-Time","NYC","https://github.com/Vibhav-Misra/will-i-catch-that-train","https://huggingface.co/spaces/VibzMiz/will-i-catch-that-train","‚óâ A Real-Time J/Z/M NYC Subway Tracker & Commute Optimizer\r\n\r\n‚óâ A Streamlit web app that helps you decide when to leave home for the J, Z, M subway lines in NYC.  \r\n\r\n‚óâ It combines static GTFS schedules with real-time GTFS-RT feeds to draw trains moving on the map and to compute a ‚ÄúLeave-Now‚Äù suggestion.\r\n\r\n---\r\n\r\n‚óâ **Features**\r\n- **Static Map** Plots only J/Z/M routes on a dark map (via Folium / Pydeck). \r\n- **Live Positions** Interpolates train positions between stops using live arrival times (no GPS required). \r\n- **Leave-Now Assistant** Calculates when to leave home based on walking time + buffer vs upcoming train arrivals. \r\n- **Platform-specific Stop IDs** Supports separate northbound / southbound platforms (e.g., `M11S`). \r\n- **Feed-agnostic** Works without API keys ‚Äî uses the public MTA GTFS-RT endpoints. \r\n- **Deploy-ready** Runs locally or on Hugging Face Spaces as a Streamlit app. \r\n\r\n‚óâ **Data Sources**\r\n\r\n- Static GTFS ‚Üí schedules, stops, shapes (downloaded zip you placed in data/).\r\n\r\n- Realtime feeds ‚Üí public MTA endpoints.\r\n\r\n- J/Z ‚Üí https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/nyct/gtfs-jz.\r\n\r\n- B/D/F/M (for M) ‚Üí https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/nyct/gtfs-bdfm.\r\n\r\n- Some CDN edges still return 403 for the slash-based URL; the code automatically retries with the %2F-encoded URL.\r\n\r\n‚óâ **Tech Stack**\r\n- Frontend / UI: Streamlit, Pydeck (or Folium)\r\n- Realtime data: MTA GTFS-RT (protobuf via gtfs-realtime-bindings)\r\n- Static data: MTA GTFS static bundle (CSV in zip)\r\n- Computation: Python 3.9+, pandas, numpy, shapely\r\n- Hosting: HuggingFace Spaces\r\n- Version control: Git + GitHub / HF Repo\r\n\r\n‚óâ **Workflow**\r\n\u003Cp style=\"margin-top:1rem;\">\u003C/p>\r\n```bash\r\n### 1. Clone & enter project\r\ngit clone https://github.com/\u003Cyour-username>/will-i-catch-that-train.git\r\ncd will-i-catch-that-train\r\n\r\n### 2. Set up Python env\r\npython -m venv venv\r\nvenv\\Scripts\\activate       \r\npip install -r requirements.txt\r\n\r\n### 3. Place the static GTFS bundle\r\n# Download the latest MTA Subway static GTFS zip from https://new.mta.info/developers\r\n# Put the untouched zip file as:\r\ndata/nyc_gtfs_static.zip\r\n\r\n### 4. Run the app\r\nstreamlit run streamlit_app.py","src/content/projects/will-i-catch-that-train.mdx","23851ca43e9727b1","will-i-catch-that-train.mdx","blog",["Map",79,80,89,90,99,100],"data-sources",{"id":79,"data":81,"body":85,"filePath":86,"digest":87,"legacyId":88,"deferredRender":28},{"title":82,"date":83,"description":84},"Where I Find My Data",["Date","2025-10-05T04:00:00.000Z"],"A curated list of all the data sources that I use for my EDA, ML, and visualization experiments.","Hey there!\r\n\r\nThroughout my studies, my professors have always emphasized the importance of choosing the right data source for any project, it can shape the entire outcome of a data science workflow.\r\n\r\nData often accounts for about 70% of a project‚Äôs predictive power, while the model contributes the remaining 30%. In other words, a strong dataset is crucial.\r\n\r\nSo, for this blog post, I decided to share some of the data sources I use most often in my projects, from classic repositories to a few niche finds.\r\n\r\n1) Kaggle Datasets - https://www.kaggle.com/datasets\r\n\r\n- Anyone who has worked with data is probably familiar with Kaggle. I find myself coming back to it regularly, whether for class projects or just exploring for future ideas.\r\n- For each dataset, you‚Äôll often find user-contributed notebooks, EDA reports, ML models, and visualizations which really helps a lot!\r\n\r\n2) UC Irvine Machine Learning Repository - https://archive-beta.ics.uci.edu/\r\n\r\n- A classic collection of datasets, domain theories, and data generators used for decades by the ML community. \r\n- It has been widely used by students, educators, and researchers all over the world as a primary source of machine learning datasets.\r\n\r\n3) Registry of Open Data on AWS - https://registry.opendata.aws/\r\n\r\n- Helps people discover and share datasets stored on AWS.\r\n- When data is shared on AWS, anyone can analyze it and build services on top of it using a broad range of compute and data analytics products, including Amazon EC2, Amazon Athena, AWS Lambda, and Amazon EMR. Sharing data in the cloud lets data users spend more time on data analysis rather than data acquisition.\r\n- You can also submit your own project and may even get featured.\r\n\r\n4) Google Dataset Search - https://datasetsearch.research.google.com/\r\n\r\n- A search engine for datasets across thousands of web repositories.\r\n- Great for quick discovery when you have a specific topic in mind.\r\n\r\n5) Microsoft Research Open Data - https://www.microsoft.com/en-us/research/tools/\r\n\r\n- An index of datasets, SDKs, APIs, and open-source tools developed by Microsoft researchers.\r\n- Particularly useful for academic and research-oriented projects.\r\n\r\n6) Github: awesome-public-datasets - https://github.com/awesomedata/awesome-public-datasets\r\n\r\n- A community-curated list of topic-specific public datasets, collected and tidied from blogs, answers, and user responses. \r\n- Most are free, and it‚Äôs a great place to stumble upon something unexpected.\r\n\r\n7) Open Government Data Platform (OGD) India - https://www.data.gov.in/\r\n\r\n- Hosted by the National Informatics Centre (NIC) under India‚Äôs Ministry of Electronics & IT.\r\n- Contains a wide range of datasets from Indian government sources.\r\n\r\n8) U.S. Government's Open Data - https://data.gov/\r\n\r\n- The U.S. federal government‚Äôs central open data hub.\r\n- Includes everything from demographic data to environmental, health, and economic datasets.\r\n\r\n9) OpenDataNI - https://www.opendatani.gov.uk/\r\n\r\n- Features datasets from public-sector organizations in Northern Ireland.\r\n\r\n10) The official portal for European data - https://data.europa.eu/en\r\n\r\n- A single access point for open data from across the EU, from international, national, regional, local and geodata portals. \r\n\r\n11) Airbnb Data Portal - https://www.airroi.com/data-portal/\r\n\r\n- One of my most interesting finds.\r\n- Offers comprehensive Airbnb data worldwide through downloadable datasets and real-time API endpoints which makes it great for market analysis and ROI studies.\r\n\r\nData sources can make or break a project. Exploring them not only sparks new project ideas but also gives you a better sense of the data‚Äôs quality and context.\r\n\r\nIf you have a favorite source I missed, I‚Äôd love to hear about it!\r\n\r\n\r\n~Vibhav","src/content/blog/data-sources.mdx","7c9a9852a56bbd39","data-sources.mdx","hello-world",{"id":89,"data":91,"body":95,"filePath":96,"digest":97,"legacyId":98,"deferredRender":28},{"title":92,"date":93,"description":94},"Hello, World",["Date","2025-10-01T04:00:00.000Z"],"Why I tore down my old portfolio and rebuilt it with Astro, Tailwind, and MDX.","Welcome to the new site!  \r\n\r\nFor a while my old portfolio felt‚Ä¶ like a college dorm room. It worked, but it was cramped, messy, and every time I wanted to add a new project I had to shove another box into the corner. It was time for an upgrade.\r\n\r\n## Why rebuild (again)?\r\n\r\nTwo reasons:\r\n\r\n1. **Flexibility.** I wanted a place where I could post not just polished projects but also share jupyter noteboks, code snippets, visualizations and also write blogs.\r\n2. **Modern stack.** I‚Äôd been curious about Astro and wanted to see how it feels to build a site that‚Äôs fast, simple to deploy, and friendly to both static and dynamic content.\r\n\r\n## Stack choices \r\n\r\n- **Astro** -> For its island architecture and smooth content collections. It lets me write blog posts in MDX and render project cards without fighting a huge frontend framework.\r\n- **Tailwind CSS** -> I used to write custom CSS for everything; now I just sprinkle utility classes and focus on layout and design.\r\n- **MDX** -> Because plain Markdown is great‚Ä¶ until you want to drop in a chart or a custom React component. MDX keeps things flexible.\r\n\r\nI also wired up content collections so blog posts and project pages can live alongside their data, which means no more copy-pasting routes or templates.\r\n\r\n## What you‚Äôll find here\r\n\r\nI‚Äôll be using this space to:\r\n- Share write-ups of data science projects (with code, visuals, and even the mistakes I made).\r\n- Post notebooks or EDA walkthroughs that deserve more explanation than a README.\r\n- Occasionally write about tools, workflows, or just what I‚Äôm learning.\r\n\r\nThe goal is to make this site feel more like a working lab notebook than a static resume.\r\n\r\nUntil then, thanks for stopping by.  \r\nIf something looks broken, that‚Äôs probably just me tinkering again, so please bear with me :)\r\n\r\n~Vibhav","src/content/blog/hello-world.mdx","c2862288258a2cae","hello-world.mdx","eda-workflow",{"id":99,"data":101,"body":105,"filePath":106,"digest":107,"legacyId":108,"deferredRender":28},{"title":102,"date":103,"description":104},"How I Approach EDA - My Personal Workflow",["Date","2025-11-18T00:00:00.000Z"],"A walkthrough of my personal EDA workflow -> the questions I ask, the steps I follow, the mistakes I‚Äôve learned from, and how I turn raw datasets into real insights.","**Hello!**\r\n\r\nIf you‚Äôve read my earlier posts, you know two things about me already:\r\n1) I rebuilt my portfolio because I wanted a place to share my work as I actually do it.\r\n2) All the websites that I frequently visit to look for a suitable dataset for my work.\r\n\r\nSo, in this blog post, I decided to write about the next logical step, which is ‚Äì \r\n\r\nWhat I actually do once I have a dataset in my hands. \r\n\r\nThis is the workflow I follow every time (or at least try to, because every so often I did come across such a problem statement, where this order of steps will not exactly work, and I had to make some tweaks, or find new ways to get moving); whether I‚Äôm building a machine learning model, exploring a new topic, or just doing a late-night ‚Äúlet me see what this dataset looks like‚Äù session.\r\n\r\nThis isn‚Äôt meant to be a perfect or universal process.\r\n\r\nIt‚Äôs MY process, one that evolved through many mistakes, many notebooks, and many confusing CSVs.\r\n\r\nLet‚Äôs get into it.\r\n\r\n## 1. Start With a Question, Not the Data\r\n\r\nFor years I used to open a dataset and immediately dive into `df.describe()` and `df.info()`.\r\n\r\nIt worked but I always ended up lost.\r\n\r\nNow I start with something much simpler.\r\n\r\n‚ÄúWhat do I want to learn from this dataset?‚Äù\r\n\r\n**Examples:**\r\n\r\n- ‚ÄúIs there a pattern in customer churn?‚Äù\r\n- ‚ÄúHow do NYC subway delays vary by line?‚Äù\r\n- ‚ÄúWhich exoplanets look remotely habitable?‚Äù\r\n- ‚ÄúCan I predict house prices with a simple model?‚Äù\r\n\r\nThis question becomes my guide and helps me navigate the process.\r\n\r\n## 2. Get the Lay of the Land (The First 10 Minutes)\r\n\r\nThis is my warm-up routine.\r\n```python\r\ndf.head()\r\ndf.info()\r\ndf.describe(include='all')\r\n# Use df.describe(include='object') or df.describe(include='category') depending on context.\r\ndf.shape\r\n```\r\nI purposely don‚Äôt try to understand everything yet.\r\n\r\nI‚Äôm just trying to get a sense of:\r\n\r\n- How big is this dataset?\r\n- Are there categorical columns?\r\n- Do the numeric columns look reasonable?\r\n- Are there weird values that should not exist?\r\n\r\nIt‚Äôs like walking through a new apartment before deciding where the furniture goes.\r\n\r\n## 3. My Mental EDA Checklist\r\n\r\nThis took me the longest time to develop, but now I try to stick to it.\r\n\r\nThink of it as:\r\n\r\n- What I check, in this order, always.\r\n\r\n**1) Missing Values -**\r\n\r\nI start with a simple table:\r\n\r\n```python\r\ndf.isnull().sum()\r\n```\r\n\r\nQuestions I ask myself:\r\n\r\n- Are the missing values meaningful or just noise?\r\n- Should I drop them, fill them, or create a new feature?\r\n\r\nI will later on write a separate blog post about handling missing values, because of its significance.\r\n\r\nI would also like to point out that missing values do not always hurt model performance and some tree-based models (XGBoost, LightGBM, CatBoost) handle them natively.\r\n\r\n**2) Data Types -**\r\n\r\nThis sounds boring, but it has saved me countless headaches.\r\n\r\n- Are dates stored as strings?\r\n- Are IDs stored as numbers?\r\n- Are categorical columns numeric?\r\n\r\nIf something feels wrong, it usually is. \r\n\r\nAlso, majority of the publicly available datasets have information section somewhere near the download link, which tells us the feature, as well as its data type and a brief description. So just make sure to refer to that!\r\n\r\n**3) Basic Distributions -**\r\n\r\nHistograms, box plots, KDEs, basically anything that helps me see the shape of the data.\r\nYou‚Äôd be surprised how often a single plot changes your entire approach.\r\n\r\n**4) Outliers -**\r\n\r\nOutliers are like plot twists in a movie, sometimes they‚Äôre noise, sometimes they‚Äôre the whole story.\r\n\r\nI look for:\r\n\r\n- Extremely large values\r\n- Negative values where there shouldn‚Äôt be any\r\n- Repeated patterns\r\n- Duplicates\r\n\r\nAlso, this is the point where you may want to revisit your problem statement, just to know the significance of the outliers. \r\n\r\n**5) Correlations -**\r\n\r\nNot to confirm assumptions, but to break them.\r\n\r\nThis is the snippet that I usually use to generate a correlation heatmap of all the numeric features:\r\n\r\n```python\r\ncorr = df_numeric.corr()\r\ncorr.style.background_gradient(cmap='coolwarm')\r\n```\r\n\r\nAn example visualization, as you can see, colored cells as well as the numeric values gives a really good idea of the correlation between the features:\r\n\r\n![Correlation Heatmap](/images/visualizations/heatmap.png)\r\n\r\nYou can also use VIF (Variance Inflation Factor), to detect multicollinearity in regression settings.\r\n\r\n**6) Feature Relationships -**\r\n\r\nScatter plots, pairplots, groupby tables.\r\n\r\nThis is where ideas start forming.\r\n\r\n## 4. The Part Everyone Forgets: Asking New Questions\r\n\r\nEDA isn‚Äôt linear.\r\n\r\nIt‚Äôs exploratory for a reason.\r\n\r\nWhen I find something interesting, I pause and ask:\r\n\r\n- Why is this column distributed like this?\r\n- Is this pattern consistent across groups?\r\n- What happens if I isolate top/bottom 5%?\r\n- Does this feature depend on time?\r\n\r\nThese new questions often become mini-projects inside the project, and trust me, this part is super important, because it‚Äôs this curiosity that makes any project stand apart. \r\n\r\n## 5. Real Examples (The Fun Part)\r\n\r\nThese are some genuine ‚Äúbefore/after‚Äù moments that changed my approach:\r\n\r\n**1) Example 1: Customer Churn Dataset -**\r\n\r\n**Before:** I assumed monthly charges affected churn linearly.\r\n\r\n**After:** A scatter plot showed a bimodal distribution, there were two types of customers entirely.\r\n\r\nThat plot alone changed my whole modeling approach.\r\n\r\n**2) Example 2: Airbnb NYC -**\r\n\r\n**Before:** I thought ‚Äúprice‚Äù would correlate strongly with location.\r\n\r\n**After:** The strongest correlation was actually with ‚Äúminimum nights‚Äù and ‚Äúavailability.‚Äù\r\n\r\nDepending on the version of the dataset and filters, correlations vary, but in my analysis, minimum nights and availability came out strongest.\r\n\r\n**3) Example 3: Exoplanets Dataset -**\r\n\r\n**Before:** I assumed habitability would depend most on temperature.\r\n\r\n**After:** Stellar radius affects luminosity and thus habitability indirectly.\r\n\r\n## 6. Mistakes I Made When I Started (That You Can Avoid)\r\n\r\nI want this blog to feel honest and helpful, so here are the mistakes that cost me the most time:\r\n\r\n**Mistake 1: Diving into modeling too fast**\r\n\r\n- You waste time tuning a model that‚Äôs learning nothing.\r\n\r\n**Mistake 2: Ignoring domain context**\r\n\r\n- Just because the numbers look fine doesn‚Äôt mean they mean something.\r\n\r\n**Mistake 3: Trusting correlation too much**\r\n\r\n- Correlation is a hint, not a conclusion. (Correlation is NOT causation!!!!)\r\n\r\n**Mistake 4: Cleaning data blindly**\r\n\r\n- Sometimes a ‚Äúweird value‚Äù is actually the key insight.\r\n\r\n**Mistake 5: Not documenting anything**\r\n\r\n- Now I literally leave a trail of markdown comments in my notebooks.\r\n\r\n## 7. When I Know I‚Äôm ‚ÄúDone‚Äù With EDA\r\n\r\nHonestly‚Ä¶ I‚Äôm never fully done.\r\n\r\nBut I stop exploring and start modeling when:\r\n\r\n- I understand the distribution of key variables\r\n- I know which features matter and which don‚Äôt\r\n- I‚Äôve identified major issues (missing values, outliers, leakage)\r\n- I can clearly articulate the story of the dataset\r\n- The assignment is near to the submission deadline :(\r\n\r\nEDA gives me a mental map.\r\n\r\nModeling is just walking through it.\r\n\r\n## 8. Final Thoughts\r\n\r\nThis workflow isn‚Äôt about rigid steps, it‚Äôs about curiosity.\r\n\r\nEDA is the part of data science where you discover things, question your assumptions, and let the dataset surprise you.\r\n\r\nIf you‚Äôre new to data science, I hope this post gives you a starting point.\r\n\r\nIf you‚Äôre more experienced, I hope it feels familiar.\r\n\r\nIn future posts, I‚Äôll dive deeper into some of these EDA steps and also discuss some of my recent projects, how I explored them, the mistakes I made, and the insights that changed the direction of the work.\r\n\r\nIf you made it this far, here‚Äôs a cookie üç™\r\n\r\n~Vibhav","src/content/blog/eda-workflow.mdx","a6907adc3b257f06","eda-workflow.mdx"]